{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b4b46d-c570-4d24-9566-2b60c0bc813d",
   "metadata": {},
   "source": [
    "# 0.) Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad62a8a9-2f39-45b4-a28b-50a10be51393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import kfp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import PipelineConf, data_passing_methods\n",
    "from kubernetes.client.models import V1Volume, V1PersistentVolumeClaimVolumeSource\n",
    "import os\n",
    "from pydoc import importfile\n",
    "import requests\n",
    "from tensorflow import keras\n",
    "from typing import List\n",
    "\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151dd754-277c-4560-867a-03bf54727b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blackboard': 'artefacts',\n",
       " 'model_name': 'fraud-detection',\n",
       " 'cluster_configuration_secret': '',\n",
       " 'training_gpus': '1',\n",
       " 'training_node_selector': ''}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_IMAGE = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\"\n",
    "\n",
    "COMPONENT_CATALOG_FOLDER = f\"{os.getenv('HOME')}/components\"\n",
    "COMPONENT_CATALOG_GIT = \"https://github.com/lehrig/kubeflow-ppc64le-components.git\"\n",
    "COMPONENT_CATALOG_RELEASE = \"main\"\n",
    "\n",
    "load_dataframe_via_trino_comp = kfp.components.load_component_from_file(\n",
    "    \"component.yaml\"\n",
    ")\n",
    "\n",
    "ARGUMENTS = {\n",
    "    \"blackboard\": \"artefacts\",\n",
    "    \"model_name\": \"fraud-detection\",\n",
    "    \"cluster_configuration_secret\": os.getenv(\n",
    "        \"CLUSTER_CONFIGURATION_SECRET\", default=\"\"\n",
    "    ),\n",
    "    \"training_gpus\": os.getenv(\"TRAINING_GPUS\", default=\"1\"),\n",
    "    \"training_node_selector\": os.getenv(\"TRAINING_NODE_SELECTOR\", default=\"\"),\n",
    "}\n",
    "MODEL_NAME = ARGUMENTS[\"model_name\"]\n",
    "\n",
    "with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\") as f:\n",
    "    NAMESPACE = f.read()\n",
    "\n",
    "ARGUMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd5a63f-387e-4539-b268-6851eb1e8ab8",
   "metadata": {},
   "source": [
    "# 1.) Load Catalog with Reusable KF components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cffdb8d1-bdac-4cd9-bf9b-6320ea2a03bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path '/home/jovyan/components' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone --branch $COMPONENT_CATALOG_RELEASE $COMPONENT_CATALOG_GIT $COMPONENT_CATALOG_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28becbc4-ec16-41b1-befc-635e4c11b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = importfile(f\"{COMPONENT_CATALOG_FOLDER}/catalog.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec27553-57ff-4bdb-8f7f-a69719018843",
   "metadata": {},
   "source": [
    "# 2.) Create custom components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da1582-1fe4-47b0-a602-fdb2f2b1d5b8",
   "metadata": {},
   "source": [
    "## 2.1) Component: Preprocess data (dataset loading, rebalancing & splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b777d42-e9a6-4ea3-89f3-2db8768c0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(\n",
    "    dataframe: InputPath(str),\n",
    "    validation_dataset_dir: OutputPath(str),\n",
    "    train_dataset_dir: OutputPath(str),\n",
    ") -> List[str]:\n",
    "\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    import math\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import (\n",
    "        LabelEncoder,\n",
    "        OneHotEncoder,\n",
    "        FunctionTransformer,\n",
    "        MinMaxScaler,\n",
    "        LabelBinarizer,\n",
    "    )\n",
    "    from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "    def save_to_dir(x, y, directory):\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        np.savez(os.path.join(directory, \"data.npz\"), x=x, y=y)\n",
    "\n",
    "    def split_dataset(n, df):\n",
    "        test = df.iloc[:n, :]\n",
    "        train = df.iloc[n:, :]\n",
    "        return test, train\n",
    "\n",
    "    def merge_splits(frauds, non_frauds, split):\n",
    "        print(\n",
    "            f\"{split} ratio fraud ({len(frauds)}) / non-fraud ({len(non_frauds)}):\",\n",
    "            len(frauds) / len(non_frauds),\n",
    "        )\n",
    "        df = pd.concat([frauds, non_frauds])\n",
    "        df.sort_values(\"year_month_day_time\", inplace=True)\n",
    "\n",
    "        x, y = df.drop([\"is fraud?\"], axis=1), df[\"is fraud?\"]\n",
    "        min_ind = math.floor(len(x) / 128)\n",
    "        x, y = x[-min_ind * 128 :], y[-min_ind * 128 :]\n",
    "        y = y.astype(\"int\")\n",
    "        return x, y\n",
    "\n",
    "    def timeEncoder(X):\n",
    "        X_hm = X[\"time\"].str.split(\":\", expand=True)\n",
    "        d = pd.to_datetime(\n",
    "            dict(\n",
    "                year=X[\"year\"],\n",
    "                month=X[\"month\"],\n",
    "                day=X[\"day\"],\n",
    "                hour=X_hm[0],\n",
    "                minute=X_hm[1],\n",
    "            )\n",
    "        ).astype(int)\n",
    "        return pd.DataFrame(d)\n",
    "\n",
    "    def amtEncoder(X):\n",
    "        amt = (\n",
    "            X.apply(lambda x: x[1:])\n",
    "            .astype(float)\n",
    "            .map(lambda amt: max(1, amt))\n",
    "            .map(math.log)\n",
    "        )\n",
    "        return pd.DataFrame(amt)\n",
    "\n",
    "    def decimalEncoder(X, length=5):\n",
    "        dnew = pd.DataFrame()\n",
    "        for i in range(length):\n",
    "            dnew[i] = np.mod(X, 10)\n",
    "            X = np.floor_divide(X, 10)\n",
    "        return dnew\n",
    "\n",
    "    def fraudEncoder(X):\n",
    "        return np.where(X == \"Yes\", 1, 0).astype(int)\n",
    "\n",
    "    # df_nf = pd.read_csv(f\"{os.getenv('HOME')}/card_transactions_non-frauds.csv\")\n",
    "    # df_f = pd.read_csv(f\"{os.getenv('HOME')}/card_transactions_frauds.csv\")\n",
    "    # tdf = pd.concat([df_nf, df_f])\n",
    "    print(\"read in raw data\")\n",
    "    tdf = pd.read_feather(dataframe)\n",
    "    tdf.columns = map(str.lower, tdf.columns)\n",
    "    tdf[\"merchant name\"] = tdf[\"merchant name\"].astype(str)\n",
    "    tdf.drop([\"mcc\", \"zip\", \"merchant state\"], axis=1, inplace=True)\n",
    "    tdf.sort_values(by=[\"user\", \"card\"], inplace=True)\n",
    "    tdf.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    mapper = DataFrameMapper(\n",
    "        [\n",
    "            (\"is fraud?\", FunctionTransformer(fraudEncoder)),\n",
    "            (\n",
    "                \"merchant name\",\n",
    "                [LabelEncoder(), FunctionTransformer(decimalEncoder), OneHotEncoder()],\n",
    "            ),\n",
    "            (\n",
    "                \"merchant city\",\n",
    "                [LabelEncoder(), FunctionTransformer(decimalEncoder), OneHotEncoder()],\n",
    "            ),\n",
    "            ([\"use chip\"], [SimpleImputer(strategy=\"constant\"), LabelBinarizer()]),\n",
    "            ([\"errors?\"], [SimpleImputer(strategy=\"constant\"), LabelBinarizer()]),\n",
    "            (\n",
    "                [\"year\", \"month\", \"day\", \"time\"],\n",
    "                [FunctionTransformer(timeEncoder), MinMaxScaler()],\n",
    "            ),\n",
    "            (\"amount\", [FunctionTransformer(amtEncoder), MinMaxScaler()]),\n",
    "        ],\n",
    "        input_df=True,\n",
    "        df_out=True,\n",
    "    )\n",
    "    print(\"fit and transform dataframe\")\n",
    "    mapper.fit(tdf)\n",
    "    tdf = mapper.transform(tdf)\n",
    "\n",
    "    dataset = tdf\n",
    "    dataset = dataset.sample(frac=1)  # shuffle randomly\n",
    "\n",
    "    frauds = dataset[dataset[\"is fraud?\"] == 1]\n",
    "    non_frauds = dataset[dataset[\"is fraud?\"] == 0]\n",
    "    ratio = len(frauds) / len(non_frauds)\n",
    "    print(\n",
    "        f\"{len(frauds)} Frauds ({len(frauds)/len(dataset)*100}%) and {len(non_frauds)} Non-Frauds ({len(non_frauds)/len(dataset)*100}%) - ratio: {ratio}).\"\n",
    "    )\n",
    "\n",
    "    test_ratio = 0.1\n",
    "    n_test_frauds = int(test_ratio * len(frauds))\n",
    "    n_test_non_frauds = int(test_ratio * len(non_frauds))\n",
    "    n_train_frauds = len(frauds) - n_test_frauds\n",
    "    n_train_non_frauds = len(non_frauds) - n_test_non_frauds\n",
    "    # n_frauds = int(0.001 * len(dataset))\n",
    "    # n_non_frauds = int(len(dataset) * 0.2 - n_frauds)\n",
    "\n",
    "    print(f\"Frauds in test split: {n_test_frauds}\")\n",
    "    test_frauds, train_frauds = split_dataset(n_test_frauds, frauds)\n",
    "\n",
    "    print(f\"Non-Frauds in test split: {n_test_non_frauds}\")\n",
    "    test_non_frauds, train_non_frauds = split_dataset(n_test_non_frauds, non_frauds)\n",
    "\n",
    "    x_train, y_train = merge_splits(train_frauds, train_non_frauds, \"Train\")\n",
    "    x_test, y_test = merge_splits(test_frauds, test_non_frauds, \"Test\")\n",
    "    print(\n",
    "        f\"Using the following y-label: {y_train.name} and x-features: {x_train.columns}\"\n",
    "    )\n",
    "\n",
    "    over_sampler = RandomOverSampler(random_state=37, sampling_strategy=0.1)\n",
    "    train_input, train_target = over_sampler.fit_resample(x_train, y_train)\n",
    "    # train_input, train_target = x_train, y_train # use this if you don't want to oversample\n",
    "    print(\n",
    "        sum(train_target == 0),\n",
    "        \"negative &\",\n",
    "        sum(train_target == 1),\n",
    "        \"positive training samples (after upsampling)\",\n",
    "    )\n",
    "    print(\n",
    "        sum(y_test == 0),\n",
    "        \"negative &\",\n",
    "        sum(y_test == 1),\n",
    "        \"positive test samples\",\n",
    "    )\n",
    "    train = pd.concat([pd.DataFrame(train_target), pd.DataFrame(train_input)], axis=1)\n",
    "    train.columns = dataset.columns\n",
    "    train.sort_values(\"year_month_day_time\", inplace=True)\n",
    "    train_input, train_target = train.drop([\"is fraud?\"], axis=1), train[\"is fraud?\"]\n",
    "\n",
    "    train_target = train_target.to_numpy().reshape(len(train_target), 1)\n",
    "    y_test = y_test.to_numpy().reshape(len(y_test), 1)\n",
    "\n",
    "    save_to_dir(train_input.to_numpy(), train_target, train_dataset_dir)\n",
    "    save_to_dir(x_test.to_numpy(), y_test, validation_dataset_dir)\n",
    "\n",
    "    print(f\"Pre-processed train dataset saved. Contents of '{train_dataset_dir}':\")\n",
    "    print(os.listdir(\"/\".join(str(train_dataset_dir).split(\"/\")[:-1])))\n",
    "    print(f\"Pre-processed test dataset saved. Contents of '{validation_dataset_dir}':\")\n",
    "    print(os.listdir(\"/\".join(str(validation_dataset_dir).split(\"/\")[:-1])))\n",
    "\n",
    "    print(train_input.columns)\n",
    "    return list(train_input.columns)\n",
    "\n",
    "\n",
    "preprocess_dataset_comp = kfp.components.create_component_from_func(\n",
    "    func=preprocess_dataset,\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"imbalanced-learn\", \"scikit-learn\", \"sklearn-pandas\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7979b7-cda8-4ab5-aa07-611c72968b93",
   "metadata": {},
   "source": [
    "## 2.2) Specification: model training & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa39bb6c-3e8a-47f6-9366-fd089ad97098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model_dir: OutputPath(str),\n",
    "    train_dataset_dir: InputPath(str),\n",
    "    validation_dataset_dir: InputPath(str),\n",
    "    epochs: int = 10,\n",
    "    seqlen: int = 7,\n",
    "):\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.callbacks import (\n",
    "        EarlyStopping,\n",
    "        ModelCheckpoint,\n",
    "        ReduceLROnPlateau,\n",
    "        TensorBoard,\n",
    "    )\n",
    "    from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "    from tensorflow.keras.metrics import (\n",
    "        TruePositives,\n",
    "        FalsePositives,\n",
    "        FalseNegatives,\n",
    "        TrueNegatives,\n",
    "    )\n",
    "\n",
    "    def load_dataset(path):\n",
    "        data = np.load(os.path.join(path, \"data.npz\"), allow_pickle=True)\n",
    "        x, y = data[\"x\"], data[\"y\"]\n",
    "        x = np.asarray(x).astype(np.float32)\n",
    "        y = np.asarray(y).astype(np.int_)\n",
    "        dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "            x, y, sequence_length=seqlen, batch_size=128\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    train_dataset = load_dataset(train_dataset_dir)\n",
    "    test_dataset = load_dataset(validation_dataset_dir)\n",
    "\n",
    "    for batch in train_dataset.take(1):\n",
    "        input_d, targets = batch\n",
    "    print(\"Input shape:\", input_d.numpy().shape, \"Target shape:\", targets.numpy().shape)\n",
    "\n",
    "    input_shape = (input_d.shape[1], input_d.shape[2])\n",
    "    inputs = Input(shape=input_shape)\n",
    "    lstm_in = LSTM(100, batch_size=7, return_sequences=True)(inputs)\n",
    "    lstm_out = LSTM(100, batch_size=7)(lstm_in)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(lstm_out)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "        TruePositives(name=\"tp\"),\n",
    "        FalsePositives(name=\"fp\"),\n",
    "        FalseNegatives(name=\"fn\"),\n",
    "        TrueNegatives(name=\"tn\"),\n",
    "    ]\n",
    "    # loss = keras.losses.BinaryFocalCrossentropy(apply_class_balancing=True)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=metrics)\n",
    "    print(model.summary())\n",
    "\n",
    "    print(\"Initializing training callbacks...\")\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"loss\", patience=20, verbose=0, mode=\"min\"),\n",
    "        ModelCheckpoint(\n",
    "            f\"{model_dir}/best_model.keras\",\n",
    "            monitor=\"loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"loss\",\n",
    "            factor=0.1,\n",
    "            patience=7,\n",
    "            verbose=1,\n",
    "            min_delta=0.0001,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        TensorBoard(\n",
    "            log_dir=os.environ[\"TENSORBOARD_S3_ADDRESS\"],\n",
    "            histogram_freq=1,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=epochs,\n",
    "        verbose=3,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    results = model.evaluate(test_dataset)\n",
    "    print(\"Evaluation Loss, Accuracy, TP, FP, FN, TN:\", results)\n",
    "    TP, FP, FN, TN = results[2:]\n",
    "    if TP != 0:\n",
    "        PR = TP / (FP + TP)\n",
    "        RE = TP / (FN + TP)\n",
    "        print(\"F1 Measure:\", 2 * (PR * RE / (PR + RE)))\n",
    "\n",
    "    model.save(model_dir)\n",
    "\n",
    "\n",
    "train_specification = kfp.components.func_to_component_text(func=train_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c130c6d0-8e82-4b18-8c10-98b141be4dc4",
   "metadata": {},
   "source": [
    "## 2.3) Component: Prediction on Test Set for Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04224364-f14b-4266-a53c-0439958a5693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model_dir: InputPath(str),\n",
    "    test_dataset_dir: InputPath(str),\n",
    "    predictions_dir: OutputPath(str),\n",
    "    seq_len: int = 7,\n",
    "):\n",
    "\n",
    "    from tensorflow import keras\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    data = np.load(os.path.join(test_dataset_dir, \"data.npz\"), allow_pickle=True)\n",
    "    x, y = data[\"x\"], data[\"y\"]\n",
    "    x = np.asarray(x).astype(np.float32)\n",
    "    y = np.asarray(y).astype(np.int_)\n",
    "    test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x, y, sequence_length=seq_len, batch_size=128\n",
    "    )\n",
    "    y = np.concatenate([b for a, b in test_dataset], axis=0)\n",
    "    model = keras.models.load_model(model_dir)\n",
    "    preds = model.predict(test_dataset)\n",
    "    preds = [str(int(pred[0] > 0.5)) + \"\\n\" for pred in preds]\n",
    "    y = [str(x[0].item()) + \"\\n\" for x in y]\n",
    "\n",
    "    if not os.path.exists(predictions_dir):\n",
    "        os.makedirs(predictions_dir)\n",
    "    with open(os.path.join(predictions_dir, \"ytrue.txt\"), \"w\") as f:\n",
    "        f.writelines(y)\n",
    "    with open(os.path.join(predictions_dir, \"ypred.txt\"), \"w\") as f:\n",
    "        f.writelines(preds)\n",
    "\n",
    "\n",
    "predict_comp = kfp.components.create_component_from_func(\n",
    "    func=predict, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d346be3-9c32-4488-99f3-1ad7312c586e",
   "metadata": {},
   "source": [
    "## 2.4) Component: Deploy to Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89954dc9-a320-44a4-aa1a-e9e3bd61bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_to_aix(\n",
    "    model_version: int = 1,\n",
    "):\n",
    "    import requests\n",
    "\n",
    "    print(\"Updating model at AIX...\")\n",
    "    response = requests.get(\n",
    "        f\"http://p114oracle.pbm.ihost.com:3000/update?model_version={model_version}\"\n",
    "    )\n",
    "    print(response.text)\n",
    "\n",
    "\n",
    "deploy_to_aix_comp = kfp.components.create_component_from_func(\n",
    "    func=deploy_to_aix, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f427ce9a-4ce3-4f65-bb62-e2d5d6a8a8c9",
   "metadata": {},
   "source": [
    "# 3.) Create the actual pipeline by combining the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2d06d93-2fea-466a-b491-fb1e4c83fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Fraud detection\",\n",
    "    description=\"An example pipeline that tries to predict fraudulent credit card transactions\",\n",
    ")\n",
    "def fraud_pipeline(\n",
    "    blackboard: str,\n",
    "    model_name: str,\n",
    "    cluster_configuration_secret: str,\n",
    "    training_gpus: int,\n",
    "    training_node_selector: str,\n",
    "):\n",
    "    create_blackboard = dsl.VolumeOp(\n",
    "        name=\"Create Artefacts Blackboard\",\n",
    "        resource_name=blackboard,\n",
    "        modes=dsl.VOLUME_MODE_RWO,\n",
    "        size=\"4Gi\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    load_dataframe_via_trino_task = load_dataframe_via_trino_comp(\n",
    "        query=\"SELECT * FROM  jtopen.demo.fraud\",\n",
    "        columns=None,\n",
    "        columns_query=\"SHOW COLUMNS FROM jtopen.demo.fraud\",\n",
    "    )\n",
    "    load_dataframe_via_trino_task.after(create_blackboard)\n",
    "\n",
    "    CATALOG.create_dataset_quality_report(\n",
    "        dataset_dir=load_dataframe_via_trino_task.outputs[\"dataframe\"],\n",
    "        dataset_type=\"df/feather\",\n",
    "    )\n",
    "\n",
    "    preprocess_dataset_task = preprocess_dataset_comp(\n",
    "        dataframe=load_dataframe_via_trino_task.outputs[\"dataframe\"]\n",
    "    )\n",
    "\n",
    "    monitor_training_task = CATALOG.monitor_training_comp()\n",
    "\n",
    "    train_parameters = {\n",
    "        \"train_dataset_dir\": \"train_dataset_dir\",\n",
    "        \"validation_dataset_dir\": \"validation_dataset_dir\",\n",
    "        \"model_dir\": \"model_dir\",\n",
    "        \"epochs\": \"2\",\n",
    "        \"seqlen\": \"4\",\n",
    "    }\n",
    "\n",
    "    train_model_task = CATALOG.train_model_comp(\n",
    "        preprocess_dataset_task.outputs[\"train_dataset_dir\"],\n",
    "        preprocess_dataset_task.outputs[\"validation_dataset_dir\"],\n",
    "        train_specification,\n",
    "        train_parameters,\n",
    "        model_name=model_name,\n",
    "        gpus=training_gpus,\n",
    "        node_selector=training_node_selector,\n",
    "        tensorboard_s3_address=monitor_training_task.outputs[\"tensorboard_s3_address\"],\n",
    "        cluster_configuration_secret=cluster_configuration_secret,\n",
    "    )\n",
    "\n",
    "    # plot_confusion_matrix_task = CATALOG.plot_confusion_matrix_comp(\n",
    "    #    input_columns=preprocess_dataset_task.outputs[\"output\"],\n",
    "    #    label_columns={\"is fraud\": [0, 1]},\n",
    "    #    test_dataset_dir=preprocess_dataset_task.outputs[\"validation_dataset_dir\"],\n",
    "    #    model_dir=train_model_task.outputs[\"model_dir\"],\n",
    "    #    seq_len=int(train_parameters[\"seqlen\"]),\n",
    "    # )\n",
    "\n",
    "    predict_task = predict_comp(\n",
    "        test_dataset_dir=preprocess_dataset_task.outputs[\"validation_dataset_dir\"],\n",
    "        model_dir=train_model_task.outputs[\"model_dir\"],\n",
    "    )\n",
    "\n",
    "    CATALOG.plot_confusion_matrix_predictions_comp(\n",
    "        predictions_dir=predict_task.outputs[\"predictions_dir\"]\n",
    "    )\n",
    "\n",
    "    convert_model_to_onnx_task = CATALOG.convert_model_to_onnx_comp(\n",
    "        train_model_task.outputs[\"model_dir\"]\n",
    "    )\n",
    "\n",
    "    upload_model_task = CATALOG.upload_model_comp(\n",
    "        file_dir=convert_model_to_onnx_task.outputs[\"onnx_model_dir\"],\n",
    "        project_name=model_name,\n",
    "    )\n",
    "\n",
    "    CATALOG.deploy_model_with_kserve_comp(\n",
    "        project_name=model_name,\n",
    "        model_version=upload_model_task.outputs[\"model_version\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# deploy_to_aix_comp(upload_model_task.outputs[\"model_version\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b2ffe-b2b5-4972-a645-a041e8162161",
   "metadata": {},
   "source": [
    "# 4.) Run the pipeline within an experiment\n",
    "reate a pipeline run, using a pipeline configuration that:\n",
    "\n",
    "- enables data passing via persistent volumes (faster than the default MinIO-based passing)\n",
    "- disables caching (which currently is not supported for data passing via volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7296f15-54c9-436e-bda7-32b9db101840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/5a8f5bcf-1dd6-4627-8126-37cc2350da48\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/ea815019-3542-40fb-ae17-f22026d511c4\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=ea815019-3542-40fb-ae17-f22026d511c4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def disable_cache_transformer(op):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    else:\n",
    "        op.add_pod_annotation(\n",
    "            name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\"\n",
    "        )\n",
    "    return op\n",
    "\n",
    "\n",
    "pipeline_conf = PipelineConf()\n",
    "# pipeline_conf.add_op_transformer(disable_cache_transformer)\n",
    "# pipeline_conf.data_passing_method = data_passing_methods.KubernetesVolume(\n",
    "#     volume=V1Volume(\n",
    "#         name=ARGUMENTS[\"blackboard\"],\n",
    "#         persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(\n",
    "#             \"{{workflow.name}}-%s\" % ARGUMENTS[\"blackboard\"]\n",
    "#         ),\n",
    "#     ),\n",
    "#     path_prefix=f'{ARGUMENTS[\"blackboard\"]}/',\n",
    "# )\n",
    "\n",
    "kfp.Client().create_run_from_pipeline_func(\n",
    "    fraud_pipeline,\n",
    "    arguments=ARGUMENTS,\n",
    "    namespace=NAMESPACE,\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d23cc06-fc86-4ef2-9c0b-24b24bf63e75",
   "metadata": {},
   "source": [
    "# 5.) Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f3bf1ce-dea7-4093-ad10-cd5e61df74b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'model',\n",
       " 'versions': ['53'],\n",
       " 'platform': 'onnxruntime_onnx',\n",
       " 'inputs': [{'name': 'input_1', 'datatype': 'FP32', 'shape': [-1, 4, 103]}],\n",
       " 'outputs': [{'name': 'dense', 'datatype': 'FP32', 'shape': [-1, 1]}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOST = f\"{MODEL_NAME}-predictor-default.{NAMESPACE}\"\n",
    "HEADERS = {\"Host\": HOST}\n",
    "MODEL_ENDPOINT = f\"http://{MODEL_NAME}-predictor-default/v2/models/model\"\n",
    "\n",
    "res_svc = requests.get(MODEL_ENDPOINT, headers=HEADERS)\n",
    "response_svc = json.loads(res_svc.text)\n",
    "response_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49a46385-5532-406b-8b78-be412d391ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 20 rows\n"
     ]
    }
   ],
   "source": [
    "def get_data_table():\n",
    "    import pandas as pd\n",
    "    from trino.dbapi import Connection\n",
    "\n",
    "    with Connection(\n",
    "        host=\"trino.trino\",\n",
    "        port=\"8080\",\n",
    "        user=\"anybody\",\n",
    "        catalog=\"jtopen\",\n",
    "        schema=\"demo\",\n",
    "    ) as conn:\n",
    "        link = conn.cursor()\n",
    "        link.execute(\"SELECT * FROM fraud LIMIT 20\")\n",
    "        return pd.DataFrame(link.fetchall())\n",
    "\n",
    "\n",
    "vdf = get_data_table()\n",
    "print(f\"Retrieved {len(vdf)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69eb22d8-4630-4aa3-9ca6-dc009b25979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 18:45:46.960170: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-09-21 18:45:46.960218: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-09-21 18:45:46.960286: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type decimal.Decimal).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x, y \u001b[38;5;241m=\u001b[39m vdf\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto_numpy(), vdf[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(vdf), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeseries_dataset_from_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_svc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m PREDICT_ENDPOINT \u001b[38;5;241m=\u001b[39m MODEL_ENDPOINT \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/infer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m10\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/utils/timeseries_dataset.py:245\u001b[0m, in \u001b[0;36mtimeseries_dataset_from_array\u001b[0;34m(data, targets, sequence_length, sequence_stride, sampling_rate, batch_size, shuffle, seed, start_index, end_index)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# For each initial window position, generates indices of the window elements\u001b[39;00m\n\u001b[1;32m    234\u001b[0m indices \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip(\n\u001b[1;32m    235\u001b[0m     (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;28mlen\u001b[39m(start_positions)), positions_ds)\n\u001b[1;32m    236\u001b[0m )\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE,\n\u001b[1;32m    243\u001b[0m )\n\u001b[0;32m--> 245\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43msequences_from_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     indices \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip(\n\u001b[1;32m    248\u001b[0m         (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;28mlen\u001b[39m(start_positions)), positions_ds)\n\u001b[1;32m    249\u001b[0m     )\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m i, positions: positions[i],\n\u001b[1;32m    251\u001b[0m         num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE,\n\u001b[1;32m    252\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/utils/timeseries_dataset.py:270\u001b[0m, in \u001b[0;36msequences_from_indices\u001b[0;34m(array, indices_ds, start_index, end_index)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msequences_from_indices\u001b[39m(array, indices_ds, start_index, end_index):\n\u001b[0;32m--> 270\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip((dataset\u001b[38;5;241m.\u001b[39mrepeat(), indices_ds))\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m steps, inds: tf\u001b[38;5;241m.\u001b[39mgather(steps, inds),\n\u001b[1;32m    273\u001b[0m         num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE,\n\u001b[1;32m    274\u001b[0m     )\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:734\u001b[0m, in \u001b[0;36mDatasetV2.from_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_tensors\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    699\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a `Dataset` with a single element, comprising the given tensors.\u001b[39;00m\n\u001b[1;32m    700\u001b[0m \n\u001b[1;32m    701\u001b[0m \u001b[38;5;124;03m  `from_tensors` produces a dataset containing only a single element. To slice\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;124;03m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:4688\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, element, name)\u001b[0m\n\u001b[1;32m   4686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   4687\u001b[0m   \u001b[38;5;124;03m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 4688\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4689\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m   4690\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure, element)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py:126\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    123\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    125\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 126\u001b[0m             \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1638\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1629\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1630\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1631\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1634\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1635\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1638\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1641\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:48\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[1;32m     47\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m as_ref  \u001b[38;5;66;03m# Unused.\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    278\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 279\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[1;32m    282\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    303\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type decimal.Decimal)."
     ]
    }
   ],
   "source": [
    "x, y = vdf.drop([0], axis=1).to_numpy(), vdf[0].to_numpy().reshape(len(vdf), 1)\n",
    "dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x, y, sequence_length=response_svc[\"inputs\"][0][\"shape\"][1], batch_size=128\n",
    ")\n",
    "\n",
    "PREDICT_ENDPOINT = MODEL_ENDPOINT + \"/infer\"\n",
    "\n",
    "for batch in dataset.take(10):\n",
    "    input_d, output_d = batch[0], batch[1]\n",
    "    for in_x, out_y in zip(input_d, output_d):\n",
    "        payload = {\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                    \"name\": response_svc[\"inputs\"][0][\"name\"],\n",
    "                    \"shape\": [\n",
    "                        1,\n",
    "                        4,\n",
    "                        111,\n",
    "                    ],  # has to match response_svc[\"inputs\"][0][\"shape\"] (except for 1. dimension)\n",
    "                    \"datatype\": response_svc[\"inputs\"][0][\"datatype\"],\n",
    "                    \"data\": in_x.numpy().tolist(),\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        res = requests.post(PREDICT_ENDPOINT, headers=HEADERS, data=json.dumps(payload))\n",
    "        response = json.loads(res.text)\n",
    "        print(response[\"outputs\"])\n",
    "        pred = response[\"outputs\"][0][\"data\"][0]\n",
    "        print(\n",
    "            f\"Actual ({out_y.numpy()[0]}) vs. Prediction ({round(pred, 3)} => {int(round(pred, 0))})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d57db-66bb-4dc9-9fc1-937e22075632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
