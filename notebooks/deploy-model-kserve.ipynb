{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf7fbd9-0e74-4b14-b36e-f4318ffc7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "\n",
    "def deploy_model_with_kserve(\n",
    "    project_name: str,\n",
    "    model_version: int,\n",
    "    explainer_type: str = None,\n",
    "    kserve_version: str = \"v1beta1\",\n",
    "    s3_bucket: str = \"projects\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Deploys a model using KServe and Trino as backend.\n",
    "\n",
    "            Parameters:\n",
    "                    project_name: Name of the project. Must be unique for the targeted namespace and conform Kubernetes naming conventions. Example: my-model.\n",
    "                    explainer_type: Type of Alibi explanation. If None, explanations are not provided. Example: AnchorTabular.\n",
    "                    kserve_version: KServe API version. Example: v1beta1.\n",
    "                    model_version: Version of the deployed model. Relevant to match explainer version to model version. Example: 1.\n",
    "                    s3_bucket: Name of the s3 bucket in which model projects reside. Example: projects.\n",
    "            Returns:\n",
    "                    endpoint: REST endpoint where the model can be queried. Example: https://my-model-user-example-com.apps.myorg.com.\n",
    "    \"\"\"\n",
    "    from kubernetes import client, config\n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1AlibiExplainerSpec\n",
    "    from kserve import V1beta1ExplainerSpec\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1TritonSpec\n",
    "    import logging\n",
    "    import sys\n",
    "\n",
    "    logging.basicConfig(\n",
    "        stream=sys.stdout,\n",
    "        level=print,\n",
    "        format=\"%(levelname)s %(asctime)s: %(message)s\",\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        model_version = int(model_version)\n",
    "    except ValueError:\n",
    "        logging.warning(\n",
    "            \"Could not parse model version. Continuing with default value 1...\"\n",
    "        )\n",
    "        model_version = 1\n",
    "\n",
    "    # See: https://www.kubeflow.org/docs/external-add-ons/kserve/first_isvc_kserve/\n",
    "    print(\"Initializing environment...\")\n",
    "    config.load_incluster_config()\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "    api_version = constants.KSERVE_GROUP + \"/\" + kserve_version\n",
    "    storage_uri: str = f\"s3://{s3_bucket}/{project_name}\"\n",
    "\n",
    "    print(\"Initializing inference service specification...\")\n",
    "    resources_spec = client.V1ResourceRequirements(\n",
    "        requests={\"cpu\": \"1000m\", \"memory\": \"8Gi\"},\n",
    "        limits={\"cpu\": \"2000m\", \"memory\": \"16Gi\"},\n",
    "    )\n",
    "\n",
    "    # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1TritonSpec/\n",
    "    triton_spec = V1beta1TritonSpec(\n",
    "        args=[\"--strict-model-config=false\"],\n",
    "        runtime_version=\"22.03-py3\",\n",
    "        storage_uri=storage_uri,\n",
    "        resources=resources_spec,\n",
    "    )\n",
    "\n",
    "    # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1PredictorSpec/\n",
    "    predictor_spec = V1beta1PredictorSpec(\n",
    "        service_account_name=\"kserve-inference-sa\", triton=triton_spec\n",
    "    )\n",
    "\n",
    "    if explainer_type:\n",
    "        print(\"Found an explainer, which will be co-deployed.\")\n",
    "        # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1AlibiExplainerSpec/\n",
    "        alibi_spec = V1beta1AlibiExplainerSpec(\n",
    "            type=explainer_type,\n",
    "            storage_uri=f\"{storage_uri}/explainer/{model_version}\",  # /explainer.alibi\",\n",
    "            resources=resources_spec,\n",
    "        )\n",
    "\n",
    "        # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1ExplainerSpec/\n",
    "        explainer_spec = V1beta1ExplainerSpec(\n",
    "            min_replicas=1,\n",
    "            alibi=alibi_spec,\n",
    "        )\n",
    "\n",
    "    # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1InferenceServiceSpec/#properties\n",
    "    inference_service_spec = V1beta1InferenceService(\n",
    "        api_version=api_version,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=client.V1ObjectMeta(\n",
    "            name=project_name,\n",
    "            namespace=namespace,\n",
    "            annotations={\"sidecar.istio.io/inject\": \"false\"},\n",
    "        ),\n",
    "        spec=V1beta1InferenceServiceSpec(\n",
    "            predictor=predictor_spec,\n",
    "            explainer=explainer_spec if explainer_type else None,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    kserve_client = KServeClient()\n",
    "\n",
    "    print(\"Checking for existing inference service...\")\n",
    "    try:\n",
    "        inference_service = kserve_client.get(project_name, namespace=namespace)\n",
    "        print(f\"Received: {inference_service}\")\n",
    "\n",
    "        if \"status\" in inference_service:\n",
    "            print(\"Inference service already exists.\")\n",
    "\n",
    "            print(\"Patching inference service with new model version...\")\n",
    "            kserve_client.patch(project_name, inference_service_spec)\n",
    "        else:\n",
    "            print(\"Creating inference service...\")\n",
    "            kserve_client.create(inference_service_spec)\n",
    "    except Exception:\n",
    "        print(\"Creating new inference service...\")\n",
    "        kserve_client.create(inference_service_spec)\n",
    "\n",
    "    print(\"Waiting for inference service to start...\")\n",
    "    kserve_client.get(\n",
    "        project_name, namespace=namespace, watch=True, timeout_seconds=180\n",
    "    )\n",
    "\n",
    "    print(\"Getting inference URL...\")\n",
    "    inference_response = kserve_client.get(project_name, namespace=namespace)\n",
    "    inference_url = inference_response[\"status\"][\"address\"][\"url\"]\n",
    "    print(f\"inference URL: {inference_url}\")\n",
    "\n",
    "    print(\"Finished.\")\n",
    "    return inference_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f052563b-5dbb-489b-8fa0-d72a2c062fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\n",
    "    \"blackboard\": \"artefacts\",\n",
    "    \"model_name\": \"fraud-detection-ec1d0\",\n",
    "    \"cluster_configuration_secret\": \"\",\n",
    "    \"training_gpus\": \"1\",\n",
    "    \"training_node_selector\": \"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "254afa02-69dd-4370-a53b-48bb8742c4fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KServeClient' from 'kserve' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdeploy_model_with_kserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43marguments\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m, in \u001b[0;36mdeploy_model_with_kserve\u001b[0;34m(project_name, model_version, explainer_type, kserve_version, s3_bucket)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mDeploys a model using KServe and Trino as backend.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m                endpoint: REST endpoint where the model can be queried. Example: https://my-model-user-example-com.apps.myorg.com.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkubernetes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m client, config\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkserve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KServeClient\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkserve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkserve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'KServeClient' from 'kserve' (unknown location)"
     ]
    }
   ],
   "source": [
    "deploy_model_with_kserve(arguments['model_name'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4c0f4f-9ff8-4cbb-9b9f-942c82f16d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
